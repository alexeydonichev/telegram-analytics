import sqlite3
import re
from collections import Counter

class StyleAnalyzer:
    def __init__(self, db_path='analytics.db'):
        self.db_path = db_path
    
    def analyze(self, channel):
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        c.execute('SELECT text FROM posts WHERE channel = ? AND text IS NOT NULL', (channel,))
        posts = [row[0] for row in c.fetchall() if row[0]]
        conn.close()
        
        if not posts:
            return "‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        
        result = {
            'channel': channel,
            'posts_count': len(posts),
            'structure': self._analyze_structure(posts),
            'vocabulary': self._analyze_vocabulary(posts),
            'tone': self._analyze_tone(posts),
            'formatting': self._analyze_formatting(posts),
            'hooks': self._extract_hooks(posts),
            'cta': self._extract_cta(posts),
        }
        result['prompt'] = self._generate_prompt(result)
        return result
    
    def _analyze_structure(self, posts):
        lengths = [len(p) for p in posts]
        sentences = [len(re.findall(r'[.!?]+', p)) for p in posts]
        paragraphs = [len(p.split('\n\n')) for p in posts]
        return {
            'avg_length': int(sum(lengths) / len(lengths)),
            'avg_sentences': round(sum(sentences) / len(sentences), 1),
            'avg_paragraphs': round(sum(paragraphs) / len(paragraphs), 1),
            'short': len([p for p in posts if len(p) < 300]),
            'medium': len([p for p in posts if 300 <= len(p) < 1000]),
            'long': len([p for p in posts if len(p) >= 1000])
        }
    
    def _analyze_vocabulary(self, posts):
        all_text = ' '.join(posts).lower()
        words = re.findall(r'[–∞-—è—ëa-z]+', all_text)
        stop = {'–∏', '–≤', '–Ω–∞', '—Å', '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '–Ω–µ', '–∞', '–Ω–æ', '–ø–æ', '–∫', '–∏–∑', '–∑–∞', '—Ç–æ', '–≤—Å–µ', '–æ—Ç', '—Ç–∞–∫', '–∂–µ', '–¥–ª—è', '–≤—ã', '–º—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '—è', '—Ç—ã', '—É', '–æ', '–±—ã'}
        filtered = [w for w in words if w not in stop and len(w) > 2]
        freq = Counter(filtered).most_common(30)
        
        bigrams = []
        for post in posts:
            w = re.findall(r'[–∞-—è—ëa-z]+', post.lower())
            bigrams.extend([f"{w[i]} {w[i+1]}" for i in range(len(w)-1)])
        bigram_freq = Counter(bigrams).most_common(15)
        
        return {'top_words': freq, 'top_phrases': bigram_freq}
    
    def _analyze_tone(self, posts):
        all_text = ' '.join(posts).lower()
        markers = {
            'formal': ['—É–≤–∞–∂–∞–µ–º—ã–µ', '–ø—Ä–µ–¥–ª–∞–≥–∞–µ–º', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º'],
            'casual': ['–∫—Å—Ç–∞—Ç–∏', '–Ω—É', '–≤–æ–æ–±—â–µ', '–∫–æ—Ä–æ—á–µ', '–∫—Ä—É—Ç–æ'],
            'expert': ['–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', '–¥–∞–Ω–Ω—ã–µ', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–∞–Ω–∞–ª–∏–∑'],
            'emotional': ['!', 'üî•', 'üí™', '–≤–∞—É', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å'],
            'personal': ['—è ', '–º–æ–π', '–º–æ—è', '–º–Ω–µ'],
        }
        scores = {tone: sum(all_text.count(w) for w in words) for tone, words in markers.items()}
        dominant = max(scores, key=scores.get)
        return {
            'dominant': dominant,
            'exclamations': all_text.count('!'),
            'questions': all_text.count('?'),
            'emoji': len(re.findall(r'[\U0001F300-\U0001F9FF]', ' '.join(posts)))
        }
    
    def _analyze_formatting(self, posts):
        return {
            'emoji': any(re.search(r'[\U0001F300-\U0001F9FF]', p) for p in posts),
            'lists': any(re.search(r'^[\-‚Ä¢]\s', p, re.MULTILINE) for p in posts),
            'links': sum(1 for p in posts if 'http' in p or 't.me' in p),
            'hashtags': sum(len(re.findall(r'#\w+', p)) for p in posts),
        }
    
    def _extract_hooks(self, posts):
        hooks = []
        for post in posts[:20]:
            first = post.split('\n')[0].strip()
            if len(first) > 10:
                hooks.append(first[:100])
        return hooks
    
    def _extract_cta(self, posts):
        patterns = [r'–ø–æ–¥–ø–∏—Å\w+', r'–ø–µ—Ä–µ—Ö–æ–¥\w+', r'–∂–º–∏', r'—á–∏—Ç–∞–π', r'—Å–º–æ—Ç—Ä–∏', r'–ø–∏—à–∏', r'—Å—Å—ã–ª–∫']
        ctas = []
        for post in posts:
            last = post[-200:] if len(post) > 200 else post
            for pat in patterns:
                if re.search(pat, last.lower()):
                    for sent in re.split(r'[.!?\n]', last):
                        if re.search(pat, sent.lower()) and len(sent.strip()) > 5:
                            ctas.append(sent.strip())
                            break
        return list(set(ctas))[:10]
    
    def _generate_prompt(self, a):
        s, t = a['structure'], a['tone']
        return f"""–ü–∏—à–∏ –∫–∞–∫ –∞–≤—Ç–æ—Ä –∫–∞–Ω–∞–ª–∞ @{a['channel']}.

–°–¢–†–£–ö–¢–£–†–ê: ~{s['avg_length']} —Å–∏–º–≤–æ–ª–æ–≤, {s['avg_sentences']} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {s['avg_paragraphs']} –∞–±–∑–∞—Ü–µ–≤.

–¢–û–ù: {t['dominant']}. {'–ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π!' if t['exclamations'] > a['posts_count'] else '–°–ø–æ–∫–æ–π–Ω–æ'}. {'–í–æ–ø—Ä–æ—Å—ã —á–∏—Ç–∞—Ç–µ–ª—é' if t['questions'] > a['posts_count'] else '–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è'}.

–ß–ê–°–¢–´–ï –°–õ–û–í–ê: {', '.join([w[0] for w in a['vocabulary']['top_words'][:15]])}

–§–†–ê–ó–´: {', '.join([w[0] for w in a['vocabulary']['top_phrases'][:10]])}

–•–£–ö–ò:
{chr(10).join(['‚Ä¢ ' + h for h in a['hooks'][:5]])}

CTA:
{chr(10).join(['‚Ä¢ ' + c for c in a['cta'][:5]])}"""

    def print_report(self, channel):
        r = self.analyze(channel)
        if isinstance(r, str):
            print(r)
            return
        
        print(f"\n{'='*60}")
        print(f"üìä –ê–ù–ê–õ–ò–ó –°–¢–ò–õ–Ø @{r['channel']}")
        print(f"{'='*60}")
        print(f"üìù –ü–æ—Å—Ç–æ–≤: {r['posts_count']}")
        
        s = r['structure']
        print(f"\nüìê –°–¢–†–£–ö–¢–£–†–ê:")
        print(f"   –î–ª–∏–Ω–∞: {s['avg_length']} —Å–∏–º–≤ | –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {s['avg_sentences']} | –ê–±–∑–∞—Ü–µ–≤: {s['avg_paragraphs']}")
        print(f"   –ö–æ—Ä–æ—Ç–∫–∏—Ö: {s['short']} | –°—Ä–µ–¥–Ω–∏—Ö: {s['medium']} | –î–ª–∏–Ω–Ω—ã—Ö: {s['long']}")
        
        t = r['tone']
        print(f"\nüé≠ –¢–û–ù: {t['dominant']}")
        print(f"   ! = {t['exclamations']} | ? = {t['questions']} | emoji = {t['emoji']}")
        
        print(f"\nüìö –¢–û–ü –°–õ–û–í–ê: {', '.join([w[0] for w in r['vocabulary']['top_words'][:15]])}")
        print(f"\nüîó –¢–û–ü –§–†–ê–ó–´: {', '.join([w[0] for w in r['vocabulary']['top_phrases'][:10]])}")
        
        print(f"\nü™ù –•–£–ö–ò:")
        for h in r['hooks'][:5]:
            print(f"   ‚Ä¢ {h}")
        
        print(f"\nüì¢ CTA:")
        for c in r['cta'][:5]:
            print(f"   ‚Ä¢ {c}")
        
        print(f"\n{'='*60}")
        print("ü§ñ –ü–†–û–ú–ü–¢:")
        print(f"{'='*60}")
        print(r['prompt'])

if __name__ == '__main__':
    import sys
    channel = sys.argv[1] if len(sys.argv) > 1 else 'yourfit_store'
    StyleAnalyzer().print_report(channel)
